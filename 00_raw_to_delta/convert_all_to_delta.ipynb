{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67177878-3e47-450d-b6cb-676d79fe126c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "base_path='/Volumes/workspace/synthea/synthea_datasets'\n",
    "files=dbutils.fs.ls(base_path) #list files in volume\n",
    "\n",
    "#len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d0b017f-ac41-4b01-bc33-384dfc6db112",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    file_path = file.path\n",
    "\n",
    "    # Skip folders and non-data files\n",
    "    if file.isDir():\n",
    "        continue\n",
    "\n",
    "    # CSV to Delta\n",
    "    if file_path.lower().endswith(\".csv\"):\n",
    "        print(f\"Converting CSV: {file_path}\")\n",
    "        df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "        delta_path = file_path.replace(\".csv\", \"_delta\")\n",
    "        df.write.format(\"delta\").mode(\"overwrite\").save(delta_path)\n",
    "\n",
    "    # JSON to Delta\n",
    "    elif file_path.lower().endswith(\".json\"):\n",
    "        print(f\"Converting JSON: {file_path}\")\n",
    "        df = spark.read.json(file_path)\n",
    "        delta_path = file_path.replace(\".json\", \"_delta\")\n",
    "        df.write.format(\"delta\").mode(\"overwrite\").save(delta_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c431be43-927f-4634-80ce-5bf3008c0449",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "base_path = \"/Volumes/workspace/synthea/synthea_datasets\"\n",
    "db_name = \"synthea\"\n",
    "\n",
    "\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {db_name}\")\n",
    "for item in dbutils.fs.ls(base_path):\n",
    "    if item.isDir() and item.name.lower().endswith(\"_delta/\"):\n",
    "        view_name = item.name[:-7]  # remove \"_delta\"\n",
    "        delta_path = item.path.rstrip(\"/\")\n",
    "        print(f\"Creating view: {db_name}.{view_name}\")\n",
    "        spark.sql(f\"CREATE OR REPLACE VIEW {db_name}.{view_name} AS SELECT * FROM delta.`{delta_path}`\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "052492a3-86eb-49cf-b28e-dbca1490a220",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "convert_all_to_delta",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
